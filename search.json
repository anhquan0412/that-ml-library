[
  {
    "objectID": "chart_plotting.html",
    "href": "chart_plotting.html",
    "title": "chart_plotting",
    "section": "",
    "text": "from that_ml_library.data_preprocess import *\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\nfrom sklearn.preprocessing import LabelEncoder",
    "crumbs": [
      "chart_plotting"
    ]
  },
  {
    "objectID": "chart_plotting.html#variance-inflation-factor-and-correlation",
    "href": "chart_plotting.html#variance-inflation-factor-and-correlation",
    "title": "chart_plotting",
    "section": "Variance Inflation Factor and Correlation",
    "text": "Variance Inflation Factor and Correlation\n\nsource\n\nget_vif\n\n get_vif (df:pandas.core.frame.DataFrame, plot_corr=False, figsize=(10,\n          10))\n\n*Perform variance inflation factor calculation, and optionally plot correlation matrix\nNote that your dataframe should only have numerical features to perform VIF*\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\npd.DataFrame\n\ndataframe to plot\n\n\nplot_corr\nbool\nFalse\nto plot the correlation matrix\n\n\nfigsize\ntuple\n(10, 10)\nMatplotlib figsize\n\n\n\n1. Why you should use VIF: to detect multicollinearity (more than 2 columns)\n\nCompute variance inflation factor\nThe VIF is variance inflation factor the ratio of the variance of βˆj when fitting the full model (with other features) divided by the variance of βˆj if fit on its own\n\nMin(VIF) = 1 (no collinearity)\nVIF &gt;5 or &gt;10 means high collinearity\n\n\n2. How to calculating VIF: Set the suspected collinearity feature (e.g. X1) as label, and try to predict X1 using a regression model and other features\n3. What to do with high collinearity:\n\nDrop one of them\nCombine them to create a new feature\nPerform an analysis designed for highly correlated variables, such as principal components analysis or partial least squares regression.\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/titanic.csv')\ndf_num = process_missing_values(df[['Survived','Pclass','Age','SibSp','Parch']],\n                                missing_cols='Age',strategies='median')\n\n\nget_vif(df_num,True,(5,5))\n\n\n\n\n\n\n\n\nconst       28.227667\nSurvived     1.061895\nPclass       1.173788\nAge          1.361584\nSibSp        1.351837\nParch        1.199945\ndtype: float64\n\n\n\nsource\n\n\nget_correlation_by_threshold\n\n get_correlation_by_threshold (df_corr, min_thres=0.98)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf_corr\n\n\nCorrelation DataFrame\n\n\nmin_thres\nfloat\n0.98\nminimum correlation to take\n\n\n\n\nget_correlation_by_threshold(df_num.corr(),min_thres=0)\n\n{'Pclass': {'Survived': -0.11633986928104582},\n 'Age': {'Survived': -0.11211373025858094, 'Pclass': -0.3451575619176082},\n 'SibSp': {'Survived': -0.06694288369258686,\n  'Pclass': 0.08741953046914279,\n  'Age': -0.3664840343129444},\n 'Parch': {'Survived': 0.03943462980865732,\n  'Pclass': 0.016490845192711254,\n  'Age': -0.19765444198507792,\n  'SibSp': 0.39904002232194297}}\n\n\n\nget_correlation_by_threshold(df_num.corr(),min_thres=0.3)\n\n{'Age': {'Pclass': -0.3451575619176082},\n 'SibSp': {'Age': -0.3664840343129444},\n 'Parch': {'SibSp': 0.39904002232194297}}\n\n\n\nsource\n\n\nplot_cat_correlation\n\n plot_cat_correlation (df_cat, figsize=(10, 10))\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf_cat\n\n\nDataFrame with categorical features that have been processed\n\n\nfigsize\ntuple\n(10, 10)\nMatplotlib figsize\n\n\n\nLet’s process some of the categorical features\n\nfrom sklearn.preprocessing import OrdinalEncoder\n\n\nfor c in ['Sex','Embarked']:  \n    oe= OrdinalEncoder()\n    df[c] = oe.fit_transform(df[c].values.reshape(-1,1))\n\n\ndf_cat = df[['Survived','Pclass','Sex','Embarked']]\n\n\ndf_cat.head()\n\n\n\n\n\n\n\n\nSurvived\nPclass\nSex\nEmbarked\n\n\n\n\n0\n0\n3\n1.0\n2.0\n\n\n1\n1\n1\n0.0\n0.0\n\n\n2\n1\n3\n0.0\n2.0\n\n\n3\n1\n1\n0.0\n2.0\n\n\n4\n0\n3\n1.0\n2.0\n\n\n\n\n\n\n\nCramer’s V measures association between two nominal variables.\nCramer’s V lies between 0 and 1 (inclusive). - 0 indicates that the two variables are not linked by any relation. - 1 indicates that there exists a strong association between the two variables.\n\nplot_cat_correlation(df_cat,(5,5))\n\n\n\n\n\n\n\n\n\ncat_corr = get_cat_correlation(df_cat)\nget_correlation_by_threshold(cat_corr,min_thres=0.2)\n\n{'Sex': {'Survived': 0.5650175790296367},\n 'Embarked': {'Pclass': 0.23572003899034383}}",
    "crumbs": [
      "chart_plotting"
    ]
  },
  {
    "objectID": "chart_plotting.html#evaluation-plot-for-regression-problem",
    "href": "chart_plotting.html#evaluation-plot-for-regression-problem",
    "title": "chart_plotting",
    "section": "Evaluation plot for regression problem",
    "text": "Evaluation plot for regression problem\n\nsource\n\nplot_residuals\n\n plot_residuals (model, X_trn, y_trn, X_test=None, y_test=None,\n                 qqplot=True)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel\n\n\nRegression model\n\n\nX_trn\n\n\nTraining dataframe\n\n\ny_trn\n\n\nTraining label\n\n\nX_test\nNoneType\nNone\nTesting dataframe\n\n\ny_test\nNoneType\nNone\nTesting label\n\n\nqqplot\nbool\nTrue\nTo whether plot the qqplot\n\n\n\n\ndf_reg = pd.read_csv('http://www.statsci.org/data/general/uscrime.txt',sep='\\t')\n\n\nfrom sklearn.linear_model import LinearRegression\n\n\nreg_model = LinearRegression()\nreg_model.fit(df_reg.drop('Crime',axis=1), df_reg.Crime.values)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\n\nplot_residuals(reg_model, df_reg.drop('Crime',axis=1), df_reg.Crime.values, \n               X_test=None, y_test=None, qqplot=True)\n\n/home/quan/miniforge3/envs/ml_dev/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\n\n\nX_train, X_test, y_train, y_test = train_test_split(df_reg.drop('Crime',axis=1), df_reg.Crime.values, \n                                                    test_size=0.2, random_state=42)\n\n\nplot_residuals(reg_model, X_train,y_train,X_test, y_test, qqplot=True)\n\n/home/quan/miniforge3/envs/ml_dev/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nplot_prediction_distribution\n\n plot_prediction_distribution (y_true, y_pred, figsize=(15, 5))\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny_true\n\n\nTrue label numpy array\n\n\ny_pred\n\n\nPrediction numpy array\n\n\nfigsize\ntuple\n(15, 5)\nMatplotlib figsize\n\n\n\n\nreg_model = LinearRegression()\nreg_model.fit(df_reg.drop('Crime',axis=1), df_reg.Crime.values)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\n\ny_pred = reg_model.predict(df_reg.drop('Crime',axis=1))\ny_true = df_reg.Crime.values\n\n\nplot_prediction_distribution(y_true,y_pred)\n\nMSE: 28828.633430503334\nRMSE: 169.789968580312\nMAE: 129.91521266409967",
    "crumbs": [
      "chart_plotting"
    ]
  },
  {
    "objectID": "chart_plotting.html#model-evaluation-curves",
    "href": "chart_plotting.html#model-evaluation-curves",
    "title": "chart_plotting",
    "section": "Model evaluation curves",
    "text": "Model evaluation curves\n\nsource\n\nplot_learning_curve\n\n plot_learning_curve (estimator, title, X, y, axes=None, ylim=None,\n                      cv=None, n_jobs=-1, scoring=None, train_sizes=[0.05,\n                      0.24, 0.43, 0.62, 0.81, 1.0], save_fig=False,\n                      figsize=(20, 5), seed=42)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nestimator\n\n\nsklearn’s classifier\n\n\ntitle\n\n\nTitle of the chart\n\n\nX\n\n\nTraining features\n\n\ny\n\n\nTraining label\n\n\naxes\nNoneType\nNone\nmatplotlib’s axes\n\n\nylim\nNoneType\nNone\ny axis range limit\n\n\ncv\nNoneType\nNone\nsklearn’s cross-validation splitting strategy\n\n\nn_jobs\nint\n-1\nNumber of jobs to run in parallel\n\n\nscoring\nNoneType\nNone\nmetric\n\n\ntrain_sizes\nlist\n[0.05, 0.24, 0.43, 0.62, 0.81, 1.0]\nList of training size portion\n\n\nsave_fig\nbool\nFalse\nTo store the chart as png in images directory\n\n\nfigsize\ntuple\n(20, 5)\nMatplotlib figsize\n\n\nseed\nint\n42\nRandom seed\n\n\n\n\ndt = DecisionTreeClassifier(criterion='entropy',random_state=42,min_samples_leaf=1)\nplot_learning_curve(dt,'Learning Curve - Decision Tree - Titanic',df_num.drop('Survived',axis=1),df_num['Survived'],\n                      cv=5,scoring='f1_macro',train_sizes=np.linspace(0.1,1,20))\n\n\n\n\n\n\n\n\n\nsource\n\n\nplot_validation_curve\n\n plot_validation_curve (estimator, title, X, y, ylim=None, cv=None,\n                        param_name=None, param_range=None, is_log=False,\n                        n_jobs=-1, scoring=None, save_fig=False,\n                        figsize=(8, 4), fill_between=True,\n                        enumerate_x=False)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nestimator\n\n\nsklearn’s classifier\n\n\ntitle\n\n\nTitle of the chart\n\n\nX\n\n\nTraining features\n\n\ny\n\n\nTraining label\n\n\nylim\nNoneType\nNone\ny axis range limit\n\n\ncv\nNoneType\nNone\nsklearn’s cross-validation splitting strategy\n\n\nparam_name\nNoneType\nNone\nName of model’s hyperparameter\n\n\nparam_range\nNoneType\nNone\nList containing range of value for param_name\n\n\nis_log\nbool\nFalse\nTo log the value in param_range, for plotting\n\n\nn_jobs\nint\n-1\nNumber of jobs to run in parallel\n\n\nscoring\nNoneType\nNone\nmetric\n\n\nsave_fig\nbool\nFalse\nTo store the chart as png in images directory\n\n\nfigsize\ntuple\n(8, 4)\nMatplotlib figsize\n\n\nfill_between\nbool\nTrue\nTo add a upper and lower one-std line for train and test curve\n\n\nenumerate_x\nbool\nFalse\nConvert categorical hyperparam to numerical, for x axis\n\n\n\n\ndt = DecisionTreeClassifier(criterion='entropy',random_state=42)\nplot_validation_curve(dt,'Val Curve - Decision Tree - Titanic',df_num.drop('Survived',axis=1),df_num['Survived'],\n                      cv=5,param_range=np.arange(1,20,1),param_name='max_depth',scoring='f1_macro')",
    "crumbs": [
      "chart_plotting"
    ]
  },
  {
    "objectID": "chart_plotting.html#tree-visualization",
    "href": "chart_plotting.html#tree-visualization",
    "title": "chart_plotting",
    "section": "Tree visualization",
    "text": "Tree visualization\n\nsource\n\nplot_tree_dtreeviz\n\n plot_tree_dtreeviz (estimator, X, y, target_name:str,\n                     class_names:list=None, tree_index=0,\n                     depth_range_to_display=None, fancy=False, scale=1.0)\n\nPlot a decision tree using dtreeviz. Note that you need to install graphviz before using this function\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nestimator\n\n\nsklearn’s classifier\n\n\nX\n\n\nTraining features\n\n\ny\n\n\nTraining label\n\n\ntarget_name\nstr\n\nThe (string) name of the target variable; e.g., for Titanic, it’s “Survived”\n\n\nclass_names\nlist\nNone\nList of names associated with the labels (same order); e.g. [‘no’,‘yes’]\n\n\ntree_index\nint\n0\nIndex (from 0) of tree if model is an ensemble of trees like a random forest.\n\n\ndepth_range_to_display\nNoneType\nNone\nRange of depth levels to be displayed. The range values are inclusive\n\n\nfancy\nbool\nFalse\nTo draw fancy tree chart (as opposed to simplified one)\n\n\nscale\nfloat\n1.0\nScale of the chart. Higher means bigger\n\n\n\n\ndt = DecisionTreeClassifier(criterion='entropy',random_state=42,class_weight=None,max_depth=3)\ndt.fit(df_num.drop('Survived',axis=1),df_num['Survived'])\n\nDecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=42) \n\n\nAfter you have installed graphviz (https://github.com/parrt/dtreeviz#installation), run these codes to run dtreeviz\n\nplot_tree_dtreeviz(dt,df_num.drop('Survived',axis=1),df_num['Survived'],\n                   target_name='Survived',\n                   class_names=['no','yes'],\n                   fancy=True,scale=1)\n\n/home/quan/miniforge3/envs/ml_dev/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n\n\n\n\n\n\n\n\n\n\nplot_tree_dtreeviz(dt,df_num.drop('Survived',axis=1),df_num['Survived'],\n                   target_name='Survived',\n                   class_names=['no','yes'],\n                   depth_range_to_display=[2,3],\n                   fancy=True,scale=1.2)\n\n/home/quan/miniforge3/envs/ml_dev/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nplot_classification_tree_sklearn\n\n plot_classification_tree_sklearn (estimator, feature_names,\n                                   class_names:list, rotate=True,\n                                   fname='tmp')\n\nPlot a decision tree classifier using sklearn. Note that this will output a png file with fname instead of showing it in the notebook\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nestimator\n\n\nsklearn’s classifier\n\n\nfeature_names\n\n\nList of names of dependent variables (features)\n\n\nclass_names\nlist\n\nList of names associated with the labels (same order); e.g. [‘no’,‘yes’]\n\n\nrotate\nbool\nTrue\nTo rotate the tree graph\n\n\nfname\nstr\ntmp\nName of the png file to save(no extension)\n\n\n\n\n# feature names (not including label)\nfeature_names = df_num.drop('Survived',axis=1).columns.values\nprint(feature_names)\n\n['Pclass' 'Age' 'SibSp' 'Parch']\n\n\nAfter you have installed graphviz (https://github.com/parrt/dtreeviz#installation), run these codes to run sklearn tree plotting\ndt = DecisionTreeClassifier(criterion='entropy',random_state=42,class_weight=None,max_depth=3)\ndt.fit(df_num.drop('Survived',axis=1),df_num['Survived'])\nplot_tree_sklearn(dt,feature_names=df_num.drop('Survived',axis=1).columns.values,\n                  class_names=['no','yes'],\n                  rotate=True,fname='tree_depth_3_titanic')\nTo show the image in notebook, create a markdown cell and type ![](images/tree_depth_3_titanic.png)",
    "crumbs": [
      "chart_plotting"
    ]
  },
  {
    "objectID": "chart_plotting.html#decision-trees-feature-importances",
    "href": "chart_plotting.html#decision-trees-feature-importances",
    "title": "chart_plotting",
    "section": "Decision Tree’s feature importances",
    "text": "Decision Tree’s feature importances\n\nsource\n\nplot_feature_importances\n\n plot_feature_importances (importances, feature_names, figsize=(20, 10),\n                           top_n=None)\n\nPlot and return a dataframe of feature importances, using sklearn’s feature_importances_ value\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nimportances\n\n\nfeature importances from sklearn’s feature_importances_ variable\n\n\nfeature_names\n\n\nList of names of dependent variables (features)\n\n\nfigsize\ntuple\n(20, 10)\nMatplotlib figsize\n\n\ntop_n\nNoneType\nNone\nShow top n features\n\n\n\n\nfeature_names = df_num.drop('Survived',axis=1).columns.values\n\n\ndt = DecisionTreeClassifier(criterion='entropy',random_state=42,class_weight=None,max_depth=5)\ndt.fit(df_num.drop('Survived',axis=1),df_num['Survived'])\n\nplot_feature_importances(dt.feature_importances_,feature_names,top_n=3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportance\n\n\nFeature\n\n\n\n\n\nPclass\n0.087868\n\n\nSibSp\n0.186577\n\n\nAge\n0.686647\n\n\n\n\n\n\n\n\nfeature_names = df_num.drop('Survived',axis=1).columns.values\ndt = DecisionTreeClassifier(criterion='entropy',random_state=42,class_weight=None,max_depth=5)\ndt.fit(df_num.drop('Survived',axis=1),df_num['Survived'])\n\nplot_permutation_importances(dt,\n                             df_num.drop('Survived',axis=1),\n                             df_num['Survived'],\n                             scoring=['f1_macro','accuracy'],\n                             top_n=3)\n\nf1_macro\n\n\n\n\n\n\n\n\n\naccuracy\n\n\n\n\n\n\n\n\n\n[         Importance       STD\n Feature                      \n Pclass     0.049733  0.024178\n SibSp      0.063374  0.018909\n Age        0.126187  0.019904,\n          Importance       STD\n Feature                      \n Pclass     0.046154  0.014841\n SibSp      0.070513  0.013446\n Age        0.133333  0.021983]\n\n\n\nfeature_names = df_num.drop('Survived',axis=1).columns.values\ndt = DecisionTreeClassifier(criterion='entropy',random_state=42,class_weight=None,max_depth=5)\ndt.fit(df_num.drop('Survived',axis=1),df_num['Survived'])\n\nplot_permutation_importances(dt,\n                             df_num.drop('Survived',axis=1),\n                             df_num['Survived'],\n                             scoring='f1_macro'\n                             )\n\nf1_macro\n\n\n\n\n\n\n\n\n\n[         Importance       STD\n Feature                      \n Parch      0.002853  0.005706\n Pclass     0.049733  0.024178\n SibSp      0.063374  0.018909\n Age        0.126187  0.019904]",
    "crumbs": [
      "chart_plotting"
    ]
  },
  {
    "objectID": "chart_plotting.html#hyperparameters-visualization",
    "href": "chart_plotting.html#hyperparameters-visualization",
    "title": "chart_plotting",
    "section": "Hyperparameters visualization",
    "text": "Hyperparameters visualization\n\nsource\n\nparams_2D_heatmap\n\n params_2D_heatmap (search_cv:dict, param1:str, param2:str,\n                    scoring:str='f1_macro', log_param1=False,\n                    log_param2=False, figsize=(20, 10), min_hm=None,\n                    max_hm=None, higher_is_better=True)\n\nPlot 2D graph of metric value for each pair of hyperparameters\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsearch_cv\ndict\n\nA dict with keys as column headers and values as columns. Typically an attribute (cv_results_) of GridSearchCV or RandomizedSearchCV\n\n\nparam1\nstr\n\nName of the first hyperparameter\n\n\nparam2\nstr\n\nName of the second hyperparameter\n\n\nscoring\nstr\nf1_macro\nMetric name\n\n\nlog_param1\nbool\nFalse\nTo log the first hyperparameter\n\n\nlog_param2\nbool\nFalse\nTo log the second hyperparameter\n\n\nfigsize\ntuple\n(20, 10)\nMatplotlib figsize\n\n\nmin_hm\nNoneType\nNone\nMinimum value for the metric to show\n\n\nmax_hm\nNoneType\nNone\nMaximum value of the metric to show\n\n\nhigher_is_better\nbool\nTrue\nSet if high metric is better\n\n\n\n\ndt = RandomForestClassifier(random_state=42)\nparam_grid={\n    'n_estimators': np.arange(2,20),\n    'min_samples_leaf': np.arange(1,80),\n}\n# Note: in order to use params_2D_heatmap, you should set scoring to a list, and set refit to False\nclf = RandomizedSearchCV(dt,param_grid,n_iter=100,\n                         scoring=['f1_macro'],n_jobs=-1,\n                         cv=5,verbose=1,random_state=42,refit=False)\nclf.fit(df_num.drop('Survived',axis=1),df_num['Survived'])\n\nFitting 5 folds for each of 100 candidates, totalling 500 fits\n\n\n/home/quan/miniforge3/envs/ml_dev/lib/python3.11/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n\n\nRandomizedSearchCV(cv=5, estimator=RandomForestClassifier(random_state=42),\n                   n_iter=100, n_jobs=-1,\n                   param_distributions={'min_samples_leaf': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]),\n                                        'n_estimators': array([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n       19])},\n                   random_state=42, refit=False, scoring=['f1_macro'],\n                   verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomizedSearchCV?Documentation for RandomizedSearchCViFittedRandomizedSearchCV(cv=5, estimator=RandomForestClassifier(random_state=42),\n                   n_iter=100, n_jobs=-1,\n                   param_distributions={'min_samples_leaf': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]),\n                                        'n_estimators': array([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n       19])},\n                   random_state=42, refit=False, scoring=['f1_macro'],\n                   verbose=1) estimator: RandomForestClassifierRandomForestClassifier(random_state=42)  RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(random_state=42) \n\n\n\nparams_2D_heatmap(clf.cv_results_,'n_estimators','min_samples_leaf',\n                  scoring='f1_macro',\n                  figsize=(20,10))\n\n/tmp/ipykernel_16177/2399879344.py:14: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n\n\n\n\n\n\n\n\n\n\nparams_2D_heatmap(clf.cv_results_,'n_estimators','min_samples_leaf',\n                  scoring='f1_macro',\n                  figsize=(20,10),min_hm=0.45)\n\n/tmp/ipykernel_16177/2399879344.py:14: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n\n\n\n\n\n\n\n\n\n\ndt = RandomForestClassifier(random_state=42)\nparam_grid={\n    'n_estimators': np.arange(2,20),\n    'min_samples_leaf': np.arange(1,80),\n    'max_features': [0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],\n}\n# Note: in order to use params_2D_heatmap, you should set scoring to a list, and set refit to False\nclf = RandomizedSearchCV(dt,param_grid,n_iter=100,\n                         scoring=['f1_macro'],n_jobs=-1,\n                         cv=5,verbose=1,random_state=42,refit=False)\nclf.fit(df_num.drop('Survived',axis=1),df_num['Survived'])\n\nFitting 5 folds for each of 100 candidates, totalling 500 fits\n\n\nRandomizedSearchCV(cv=5, estimator=RandomForestClassifier(random_state=42),\n                   n_iter=100, n_jobs=-1,\n                   param_distributions={'max_features': [0.3, 0.4, 0.5, 0.6,\n                                                         0.7, 0.8, 0.9, 1],\n                                        'min_samples_leaf': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]),\n                                        'n_estimators': array([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n       19])},\n                   random_state=42, refit=False, scoring=['f1_macro'],\n                   verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomizedSearchCV?Documentation for RandomizedSearchCViFittedRandomizedSearchCV(cv=5, estimator=RandomForestClassifier(random_state=42),\n                   n_iter=100, n_jobs=-1,\n                   param_distributions={'max_features': [0.3, 0.4, 0.5, 0.6,\n                                                         0.7, 0.8, 0.9, 1],\n                                        'min_samples_leaf': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]),\n                                        'n_estimators': array([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n       19])},\n                   random_state=42, refit=False, scoring=['f1_macro'],\n                   verbose=1) estimator: RandomForestClassifierRandomForestClassifier(random_state=42)  RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(random_state=42) \n\n\n\nparams_3D_heatmap(clf.cv_results_,\n                  'n_estimators',\n                  'min_samples_leaf',\n                  'max_features',\n                  scoring='f1_macro')",
    "crumbs": [
      "chart_plotting"
    ]
  },
  {
    "objectID": "chart_plotting.html#partial-dependency-plot",
    "href": "chart_plotting.html#partial-dependency-plot",
    "title": "chart_plotting",
    "section": "Partial Dependency Plot",
    "text": "Partial Dependency Plot\n\nsource\n\npdp_numerical_only\n\n pdp_numerical_only (model, X:pandas.core.frame.DataFrame,\n                     num_features:list, class_names:list, y_colors=None,\n                     ncols=2, nrows=2, figsize=(20, 16))\n\nPlot PDP plot for numerical dependent variables\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel\n\n\nsklearn tree model that has been trained\n\n\nX\npd.DataFrame\n\ndataframe to perform pdp\n\n\nnum_features\nlist\n\nA list of numerical features\n\n\nclass_names\nlist\n\nList of names associated with the labels (same order); e.g. [‘no’,‘yes’]\n\n\ny_colors\nNoneType\nNone\nList of colors associated with class_names\n\n\nncols\nint\n2\n\n\n\nnrows\nint\n2\n\n\n\nfigsize\ntuple\n(20, 16)\n\n\n\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/titanic.csv')\ndf = df[['Survived','Pclass','Sex','Age','SibSp','Parch','Embarked']].copy()\ndf = preprocessing_general(df,\n                           missing_cols=['Age','Embarked'],\n                           missing_vals=np.NaN,\n                           strategies=['median','most_frequent'],\n                           cat_cols='Embarked',\n                           bi_cols='Sex'\n                          )\n\n\ndf.head()\n\n\n\n\n\n\n\n\nSurvived\nPclass\nAge\nSibSp\nParch\nEmbarked_C\nEmbarked_Q\nEmbarked_S\nSex_male\n\n\n\n\n0\n0\n3\n22.0\n1\n0\n0.0\n0.0\n1.0\n1.0\n\n\n1\n1\n1\n38.0\n1\n0\n1.0\n0.0\n0.0\n0.0\n\n\n2\n1\n3\n26.0\n0\n0\n0.0\n0.0\n1.0\n0.0\n\n\n3\n1\n1\n35.0\n1\n0\n0.0\n0.0\n1.0\n0.0\n\n\n4\n0\n3\n35.0\n0\n0\n0.0\n0.0\n1.0\n1.0\n\n\n\n\n\n\n\nTo better showcase the interpretation of Partial Dependency Plot, we will reuse the Titanic dataset, but now the independent variable (the one we need to predict) will be Pclass (3 classes to predict)\n\nparams = {'n_estimators': 12, 'min_samples_leaf': 10, 'max_features': 0.8, 'class_weight': 'balanced'}\nX_trn = df.drop('Pclass',axis=1)\ny_trn = LabelEncoder().fit_transform(df['Pclass'])\ndt = RandomForestClassifier(**params)\ndt.fit(X_trn,y_trn)\n\nRandomForestClassifier(class_weight='balanced', max_features=0.8,\n                       min_samples_leaf=10, n_estimators=12)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier(class_weight='balanced', max_features=0.8,\n                       min_samples_leaf=10, n_estimators=12) \n\n\n\npdp_numerical_only(dt,X_trn,num_features=['Age','SibSp'],class_names=['pclass_1','pclass_2','pclass_3'],nrows=2,ncols=1,figsize=(6,8))\n\n\n\n\n\n\n\n\n\nsource\n\n\npdp_categorical_only\n\n pdp_categorical_only (model, X:pandas.core.frame.DataFrame,\n                       cat_feature:list, class_names:list, y_colors=None,\n                       ymax=0.5, figsize=(20, 8))\n\nPlot PDP plot for categorical dependent variables\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel\n\n\nsklearn tree model that has been trained\n\n\nX\npd.DataFrame\n\ndataframe to perform pdp\n\n\ncat_feature\nlist\n\nA single categorical feature\n\n\nclass_names\nlist\n\nList of names associated with the labels (same order); e.g. [‘no’,‘yes’]\n\n\ny_colors\nNoneType\nNone\nList of colors associated with class_names\n\n\nymax\nfloat\n0.5\n\n\n\nfigsize\ntuple\n(20, 8)\n\n\n\n\n\npdp_categorical_only(dt,X_trn,'Survived',\n                     class_names=['pclass_1','pclass_2','pclass_3'])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nplot_ice_pair\n\n plot_ice_pair (model, X:pandas.core.frame.DataFrame, pair_features:list,\n                class_idx, figsize=(10, 4))\n\nPlot ICE plot from a pair of numerical feature\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel\n\n\nsklearn tree model that has been trained\n\n\nX\npd.DataFrame\n\ndataframe to perform ice\n\n\npair_features\nlist\n\na list of only 2 features\n\n\nclass_idx\n\n\nindex of the class to plot\n\n\nfigsize\ntuple\n(10, 4)\n\n\n\n\n\n# For pclass_1\nplot_ice_pair(dt,X_trn,pair_features=['Age','SibSp'],class_idx=0,figsize=(8,3))\n\n\n\n\n\n\n\n\n\n# For pclass_2\nplot_ice_pair(dt,X_trn,pair_features=['Age','SibSp'],class_idx=1,figsize=(8,3))\n\n\n\n\n\n\n\n\n\n# For pclass_3\nplot_ice_pair(dt,X_trn,pair_features=['Age','SibSp'],class_idx=2,figsize=(8,3))",
    "crumbs": [
      "chart_plotting"
    ]
  },
  {
    "objectID": "chart_plotting.html#other-functions",
    "href": "chart_plotting.html#other-functions",
    "title": "chart_plotting",
    "section": "Other functions",
    "text": "Other functions\n\nsource\n\nplot_confusion_matrix\n\n plot_confusion_matrix (y_true:list|numpy.ndarray,\n                        y_pred:list|numpy.ndarray, labels=None)\n\nSimple function to plot the confusion matrix\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny_true\nlist | np.ndarray\n\nA list/numpy array of true labels\n\n\ny_pred\nlist | np.ndarray\n\nA list/numpy array of predictions\n\n\nlabels\nNoneType\nNone\nDisplay names matching the labels (same order).\n\n\n\n\ny_true = np.array([1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0])\ny_pred = np.array([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0])\n\n\nplot_confusion_matrix(y_true,y_pred,labels=['Not Survived','Survived'])",
    "crumbs": [
      "chart_plotting"
    ]
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "utils",
    "section": "",
    "text": "source\n\nval2list\n\n val2list (val, lsize=1)\n\nConvert an element (nonlist value) to a list of 1 element\n\nval2list(2)\n\n[2]\n\n\n\nval2list(2,lsize=5)\n\n[2, 2, 2, 2, 2]\n\n\n\nval2list([2,3])\n\n[2, 3]\n\n\n\nval2list(np.array([1,2,3]))\n\narray([1, 2, 3])\n\n\n\nsource\n\n\ncreate_dir\n\n create_dir (path_dir)",
    "crumbs": [
      "utils"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "that-ml-library",
    "section": "",
    "text": "pip install that_ml_library\nFor tree visualization function (plot_tree_dtreeviz or plot_tree_sklearn), you also need to install graphviz. Please follow the instruction here",
    "crumbs": [
      "that-ml-library"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "that-ml-library",
    "section": "",
    "text": "pip install that_ml_library\nFor tree visualization function (plot_tree_dtreeviz or plot_tree_sklearn), you also need to install graphviz. Please follow the instruction here",
    "crumbs": [
      "that-ml-library"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "that-ml-library",
    "section": "How to use",
    "text": "How to use\nPlease visit https://anhquan0412.github.io/that-ml-library/ for tutorials and documentations",
    "crumbs": [
      "that-ml-library"
    ]
  },
  {
    "objectID": "index.html#a-word-of-caution",
    "href": "index.html#a-word-of-caution",
    "title": "that-ml-library",
    "section": "A word of caution",
    "text": "A word of caution\nThis library should only be utilized solely for developing a proof of concept or prototype for your machine learning model with your specific dataset, with the aim of evaluating the model’s performance and interpretability. For deployment in a production environment, opt for a more organized methodology, such as https://scikit-learn.org/stable/modules/compose.html#pipeline",
    "crumbs": [
      "that-ml-library"
    ]
  },
  {
    "objectID": "end_to_end_example.html",
    "href": "end_to_end_example.html",
    "title": "End-to-end tutorial - Classification",
    "section": "",
    "text": "In this tutorial, we will use the Pima Indian dataset https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset.\nfrom that_ml_library.data_preprocess import *\nfrom that_ml_library.chart_plotting import *\nfrom that_ml_library.ml_helpers import *",
    "crumbs": [
      "End-to-end tutorial - Classification"
    ]
  },
  {
    "objectID": "end_to_end_example.html#load-data",
    "href": "end_to_end_example.html#load-data",
    "title": "End-to-end tutorial - Classification",
    "section": "Load data",
    "text": "Load data\n\nimport pandas as pd\nimport numpy as np\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/anhquan0412/dataset/main/diabetes.csv')\nprint(df.shape)\ndisplay(df.sample(7))\n\n(768, 9)\n\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\n92\n7\n81\n78\n40\n48\n46.7\n0.261\n42\n0\n\n\n54\n7\n150\n66\n42\n342\n34.7\n0.718\n42\n0\n\n\n288\n4\n96\n56\n17\n49\n20.8\n0.340\n26\n0\n\n\n90\n1\n80\n55\n0\n0\n19.1\n0.258\n21\n0\n\n\n98\n6\n93\n50\n30\n64\n28.7\n0.356\n23\n0\n\n\n397\n0\n131\n66\n40\n0\n34.3\n0.196\n22\n1\n\n\n574\n1\n143\n86\n30\n330\n30.1\n0.892\n23\n0\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 768 entries, 0 to 767\nData columns (total 9 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   Pregnancies               768 non-null    int64  \n 1   Glucose                   768 non-null    int64  \n 2   BloodPressure             768 non-null    int64  \n 3   SkinThickness             768 non-null    int64  \n 4   Insulin                   768 non-null    int64  \n 5   BMI                       768 non-null    float64\n 6   DiabetesPedigreeFunction  768 non-null    float64\n 7   Age                       768 non-null    int64  \n 8   Outcome                   768 non-null    int64  \ndtypes: float64(2), int64(7)\nmemory usage: 54.1 KB",
    "crumbs": [
      "End-to-end tutorial - Classification"
    ]
  },
  {
    "objectID": "end_to_end_example.html#sklearn-logistic-regression",
    "href": "end_to_end_example.html#sklearn-logistic-regression",
    "title": "End-to-end tutorial - Classification",
    "section": "Sklearn Logistic Regression",
    "text": "Sklearn Logistic Regression\n\ndf_train = X_train_processed.copy()\ndf_train['has_diabetes'] = y_train\n\n\nget_vif(df_train,plot_corr=True)\n\n\n\n\n\n\n\n\nconst                       40.937975\nPregnancies                  1.441405\nGlucose                      1.573457\nBloodPressure                1.152426\nSkinThickness                1.529585\nInsulin                      1.538192\nBMI                          1.343058\nDiabetesPedigreeFunction     1.097619\nAge                          1.577323\nhas_diabetes                 1.451467\ndtype: float64\n\n\nWith VIF, we can see that there’s not much colinearity in this dataset. Note that VIF &gt;5 or &gt;10 means high colinearity\n\nrun_logistic_regression(X_train_processed,y_train,\n                        multi_class='multinomial',\n                        solver='newton-cg',max_iter=10000)\n\n\n\n\n\n\n\n\nFeatures\nCoefficients\nCoefficients Exp\n\n\n\n\n0\nIntercept\n-4.233888\n0.014496\n\n\n1\nPregnancies\n0.976679\n2.655623\n\n\n2\nGlucose\n3.679170\n39.613487\n\n\n3\nBloodPressure\n-0.679500\n0.506870\n\n\n4\nSkinThickness\n0.217158\n1.242540\n\n\n5\nInsulin\n-0.473694\n0.622698\n\n\n6\nBMI\n3.106698\n22.347132\n\n\n7\nDiabetesPedigreeFunction\n0.881984\n2.415688\n\n\n8\nAge\n0.461467\n1.586400\n\n\n\n\n\n\n\n----------------------------------------------------------------------------------------------------\nLog loss:  0.46620685663024525\n----------------------------------------------------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0       0.81      0.90      0.85       400\n           1       0.76      0.60      0.67       214\n\n    accuracy                           0.79       614\n   macro avg       0.78      0.75      0.76       614\nweighted avg       0.79      0.79      0.79       614\n\n\n\nLogisticRegression(max_iter=10000, multi_class='multinomial', penalty=None,\n                   random_state=0, solver='newton-cg')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(max_iter=10000, multi_class='multinomial', penalty=None,\n                   random_state=0, solver='newton-cg')\n\n\nInterpretation of coefficients in logistic regression: For a coefficient β1, given a one unit increase in one of the variables (say X1), the odds of X (p(X)/1-p(X)) will be multiplied by e^β1.\nIn this case, we can tell from coefficient of Glucose that: the odd of having diabetes will be multiplied by ~40 if Glucose level is increased by 1\n\nStatsmodel\n\nrun_multinomial_statmodel(X_train_processed,y_train,add_constant=True)\n\nOptimization terminated successfully.\n         Current function value: 0.466207\n         Iterations 6\n                          MNLogit Regression Results                          \n==============================================================================\nDep. Variable:                Outcome   No. Observations:                  614\nModel:                        MNLogit   Df Residuals:                      605\nMethod:                           MLE   Df Model:                            8\nDate:                Mon, 19 Feb 2024   Pseudo R-squ.:                  0.2789\nTime:                        11:43:30   Log-Likelihood:                -286.25\nconverged:                       True   LL-Null:                       -396.97\nCovariance Type:            nonrobust   LLR p-value:                 1.909e-43\n============================================================================================\n               Outcome=1       coef    std err          z      P&gt;|z|      [0.025      0.975]\n--------------------------------------------------------------------------------------------\nconst                       -8.4678      0.805    -10.523      0.000     -10.045      -6.891\nPregnancies                  1.9534      0.624      3.129      0.002       0.730       3.177\nGlucose                      7.3584      0.855      8.609      0.000       5.683       9.034\nBloodPressure               -1.3590      0.733     -1.855      0.064      -2.795       0.077\nSkinThickness                0.4343      0.788      0.551      0.581      -1.110       1.978\nInsulin                     -0.9474      0.856     -1.107      0.268      -2.625       0.730\nBMI                          6.2135      1.150      5.405      0.000       3.960       8.467\nDiabetesPedigreeFunction     1.7640      0.755      2.335      0.020       0.283       3.245\nAge                          0.9229      0.616      1.499      0.134      -0.284       2.130\n============================================================================================\n----------------------------------------------------------------------------------------------------\nLog loss:  0.46620685662395556\n----------------------------------------------------------------------------------------------------\n              precision    recall  f1-score   support\n\n           0       0.81      0.90      0.85       400\n           1       0.76      0.60      0.67       214\n\n    accuracy                           0.79       614\n   macro avg       0.78      0.75      0.76       614\nweighted avg       0.79      0.79      0.79       614\n\n\n\n&lt;statsmodels.discrete.discrete_model.MNLogit&gt;",
    "crumbs": [
      "End-to-end tutorial - Classification"
    ]
  },
  {
    "objectID": "end_to_end_example.html#decision-tree",
    "href": "end_to_end_example.html#decision-tree",
    "title": "End-to-end tutorial - Classification",
    "section": "Decision Tree",
    "text": "Decision Tree\n\nValidation Curve\n\nfrom sklearn.tree import DecisionTreeClassifier\n\n\ndt = DecisionTreeClassifier(criterion='entropy',random_state=42,class_weight=None)\ncv = StratifiedKFold(5)\nplot_validation_curve(dt,'Val Curve - Decision Tree - PID',X_train_processed,y_train,\n                      cv=cv,param_range=np.arange(1,20,1),param_name='max_depth',scoring='f1_macro',\n                      n_jobs=-1,figsize=(6,4))\n\n\n\n\n\n\n\n\nThis is the validation curve for hyperparameter max depth, which can be used to prune a decision tree.\nFor max depth, we can observe the overfitting behavior when max depth is too high, which means the tree is able to make unlimited splits to the extreme of classifying each of the training data points correctly, thus failing to generalize. On the other hand, a small depth is enough to achieve a good CV score (from depth 1 to 5). This is a telling sign that some features have strong signals to predict diabetes than others.\n\n\nLearning curve\n\nnp.linspace(0.1,1,20)\n\narray([0.1       , 0.14736842, 0.19473684, 0.24210526, 0.28947368,\n       0.33684211, 0.38421053, 0.43157895, 0.47894737, 0.52631579,\n       0.57368421, 0.62105263, 0.66842105, 0.71578947, 0.76315789,\n       0.81052632, 0.85789474, 0.90526316, 0.95263158, 1.        ])\n\n\n\ndt = DecisionTreeClassifier(criterion='entropy',random_state=42,class_weight=None,min_samples_leaf=27)\nplot_learning_curve(dt,'DT Learning Curve',X_train_processed,y_train,cv=cv,scoring='f1_macro',\n                    train_sizes=np.linspace(0.1,1,20))\n\n\n\n\n\n\n\n\nBased on the learning curve for the best tuned decision tree, more data is better for the model to learn, as training score seems to convert, and CV score still slowly increases. However, as the gap between CV score and train score is closing, adding lots of data might not be beneficial. For the scalability of the model, the relationship between training time and fit time is linear, which makes sense as more data allows tree to grow more, which takes more time to fit and predict.\n\n\nTree plotting\nUsing Dtreeviz\n\ndt = DecisionTreeClassifier(criterion='entropy',random_state=42,class_weight=None,max_depth=3)\ndt.fit(X_train_processed,y_train)\nplot_tree_dtreeviz(dt,X_train_processed,y_train,target_name='has_diabetes',class_names=['no','yes'],\n                   fancy=True,scale=1.0)\n\n/home/quan/anaconda3/envs/dev/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n\n\n\n\n\n\n\n\n\nGlucose alone can make CV F1 score to reach 0.704, which is not far from the CV score of the best tuned model. With domain knowledge, we can confirm that high glucose is the number one indicator for diabetes, because diabetes patients are unable to effectively use insulin to break down glucose.\nUsing Sklearn tree plot\n\ndt = DecisionTreeClassifier(criterion='entropy',random_state=42,class_weight=None,max_depth=3)\ndt.fit(X_train_processed,y_train)\nplot_classification_tree_sklearn(dt,feature_names=X_train_processed.columns.values,\n                  class_names=['no','yes'],\n                  rotate=True,fname='tree_depth_3_PID')\n\nType ![](images/tree_depth_3_PID.png)",
    "crumbs": [
      "End-to-end tutorial - Classification"
    ]
  },
  {
    "objectID": "end_to_end_example.html#model-finetuning-process",
    "href": "end_to_end_example.html#model-finetuning-process",
    "title": "End-to-end tutorial - Classification",
    "section": "Model finetuning process",
    "text": "Model finetuning process\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nparam_grid = {\n    'n_estimators': np.arange(2,80),\n    'min_samples_leaf': np.arange(1,100),\n    'max_features': [0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],\n}\nsearch_cv = tune_sklearn_model('RandomForest',param_grid,X_train_processed,y_train,\n                               is_regression=False,\n                               custom_cv=5,\n                               scoring='f1_macro',\n                               random_cv_iter=100,\n                               rank_show=7,\n                               show_split_scores=True)\n\nFitting 5 folds for each of 100 candidates, totalling 500 fits\n----------\nRank 1\nParams: {'n_estimators': 4, 'min_samples_leaf': 23, 'max_features': 0.7}\nTrain scores: [0.77, 0.76, 0.78, 0.76, 0.78]\nMean train score: 0.768 +- 0.008\nTest scores:  [0.73, 0.77, 0.71, 0.7, 0.77]\nMean test score: 0.738 +- 0.029\n----------\nRank 2\nParams: {'n_estimators': 41, 'min_samples_leaf': 5, 'max_features': 0.5}\nTrain scores: [0.88, 0.87, 0.86, 0.88, 0.87]\nMean train score: 0.872 +- 0.009\nTest scores:  [0.71, 0.78, 0.71, 0.71, 0.77]\nMean test score: 0.737 +- 0.031\n----------\nRank 3\nParams: {'n_estimators': 31, 'min_samples_leaf': 10, 'max_features': 0.6}\nTrain scores: [0.83, 0.81, 0.82, 0.82, 0.81]\nMean train score: 0.819 +- 0.011\nTest scores:  [0.72, 0.77, 0.7, 0.69, 0.79]\nMean test score: 0.737 +- 0.040\n----------\nRank 4\nParams: {'n_estimators': 20, 'min_samples_leaf': 28, 'max_features': 0.5}\nTrain scores: [0.77, 0.76, 0.78, 0.76, 0.78]\nMean train score: 0.770 +- 0.008\nTest scores:  [0.75, 0.76, 0.71, 0.68, 0.78]\nMean test score: 0.735 +- 0.037\n----------\nRank 5\nParams: {'n_estimators': 18, 'min_samples_leaf': 94, 'max_features': 0.7}\nTrain scores: [0.73, 0.73, 0.74, 0.75, 0.72]\nMean train score: 0.735 +- 0.011\nTest scores:  [0.74, 0.76, 0.7, 0.71, 0.77]\nMean test score: 0.735 +- 0.026\n----------\nRank 6\nParams: {'n_estimators': 35, 'min_samples_leaf': 61, 'max_features': 0.8}\nTrain scores: [0.74, 0.74, 0.77, 0.75, 0.73]\nMean train score: 0.746 +- 0.011\nTest scores:  [0.8, 0.76, 0.69, 0.67, 0.76]\nMean test score: 0.734 +- 0.045\n----------\nRank 7\nParams: {'n_estimators': 35, 'min_samples_leaf': 8, 'max_features': 0.5}\nTrain scores: [0.84, 0.82, 0.83, 0.84, 0.82]\nMean train score: 0.832 +- 0.008\nTest scores:  [0.75, 0.76, 0.7, 0.71, 0.76]\nMean test score: 0.734 +- 0.027\n----------\nDefault Params\nMean train score: 1.0 +- 0.0\nMean test score: 0.731 +- 0.04\n\n\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.2s finished\n\n\n\nparams_3D_heatmap(search_cv,'n_estimators','min_samples_leaf','max_features','f1_macro')\n\n                                                \n\n\nBy looking at the 3D map of these three hyperparameters, we might need to exclude max_features of 1 and min_samples_leaf that is more than 80. In fact, max_features of 0.8 and 0.9 shows the most promising set of hyperparameters, thus we can now simplied our search space to this:\n\nparam_grid = {\n    'n_estimators': np.arange(2,80),\n    'min_samples_leaf': np.arange(1,85),\n    'max_features': [0.8,0.9],\n}\nsearch_cv = tune_sklearn_model('RandomForest',param_grid,X_train_processed,y_train,custom_cv=5,\n                               scoring='f1_macro',\n                               random_cv_iter=200,\n                               rank_show=2,\n                               show_split_scores=True)\n\nFitting 5 folds for each of 200 candidates, totalling 1000 fits\n----------\nRank 1\nParams: {'n_estimators': 12, 'min_samples_leaf': 12, 'max_features': 0.8}\nTrain scores: [0.8, 0.8, 0.81, 0.82, 0.8]\nMean train score: 0.806 +- 0.010\nTest scores:  [0.74, 0.79, 0.7, 0.7, 0.79]\nMean test score: 0.743 +- 0.041\n----------\nRank 2\nParams: {'n_estimators': 33, 'min_samples_leaf': 23, 'max_features': 0.8}\nTrain scores: [0.78, 0.76, 0.77, 0.77, 0.76]\nMean train score: 0.769 +- 0.007\nTest scores:  [0.75, 0.77, 0.71, 0.69, 0.79]\nMean test score: 0.742 +- 0.038\n----------\nDefault Params\nMean train score: 1.0 +- 0.0\nMean test score: 0.731 +- 0.04\n\n\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.2s finished\n\n\n\nparams_2D_heatmap(search_cv,'n_estimators','min_samples_leaf','f1_macro',\n                  figsize=(20,10),min_hm=0.7,max_hm=None)\n\n\n\n\n\n\n\n\nWe will now focus on the area near the bottom right of the graph, for grid search\n\nparam_grid = {\n    'n_estimators': np.arange(62,73),\n    'min_samples_leaf': np.arange(20,26),\n    'max_features': [0.9],\n}\nsearch_cv = tune_sklearn_model('RandomForest',\n                              param_grid,\n                              X_train_processed,\n                              y_train,\n                              scoring='f1_macro',\n                              custom_cv=5,\n                              rank_show=5)\n\nFitting 5 folds for each of 66 candidates, totalling 330 fits\n----------\nRank 1\nParams: {'max_features': 0.9, 'min_samples_leaf': 25, 'n_estimators': 72}\nTrain scores: [0.77, 0.75, 0.78, 0.76, 0.77]\nMean train score: 0.767 +- 0.009\nTest scores:  [0.74, 0.78, 0.7, 0.72, 0.78]\nMean test score: 0.745 +- 0.033\n----------\nRank 1\nParams: {'max_features': 0.9, 'min_samples_leaf': 25, 'n_estimators': 63}\nTrain scores: [0.78, 0.75, 0.77, 0.77, 0.77]\nMean train score: 0.765 +- 0.009\nTest scores:  [0.74, 0.78, 0.7, 0.72, 0.78]\nMean test score: 0.745 +- 0.033\n----------\nRank 1\nParams: {'max_features': 0.9, 'min_samples_leaf': 25, 'n_estimators': 65}\nTrain scores: [0.78, 0.75, 0.78, 0.76, 0.77]\nMean train score: 0.766 +- 0.009\nTest scores:  [0.74, 0.78, 0.7, 0.72, 0.78]\nMean test score: 0.745 +- 0.033\n----------\nRank 1\nParams: {'max_features': 0.9, 'min_samples_leaf': 25, 'n_estimators': 62}\nTrain scores: [0.77, 0.75, 0.77, 0.77, 0.77]\nMean train score: 0.765 +- 0.008\nTest scores:  [0.74, 0.78, 0.7, 0.72, 0.78]\nMean test score: 0.745 +- 0.033\n----------\nRank 5\nParams: {'max_features': 0.9, 'min_samples_leaf': 25, 'n_estimators': 67}\nTrain scores: [0.78, 0.75, 0.77, 0.76, 0.77]\nMean train score: 0.766 +- 0.011\nTest scores:  [0.74, 0.78, 0.7, 0.71, 0.78]\nMean test score: 0.743 +- 0.034\n----------\nDefault Params\nMean train score: 1.0 +- 0.0\nMean test score: 0.731 +- 0.04\n\n\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.2s finished",
    "crumbs": [
      "End-to-end tutorial - Classification"
    ]
  },
  {
    "objectID": "end_to_end_example.html#feature-importance-permutation-technique",
    "href": "end_to_end_example.html#feature-importance-permutation-technique",
    "title": "End-to-end tutorial - Classification",
    "section": "Feature importance (permutation technique)",
    "text": "Feature importance (permutation technique)\n\nparams= {'max_features': 0.9, 'min_samples_leaf': 25, 'n_estimators': 62}\n# Mean train score: 0.765 +- 0.008\n# Mean test score: 0.745 +- 0.033\n\n\n# # turn off warnings\n# def warn(*args, **kwargs):\n#     pass\n# import warnings\n# warnings.warn = warn\n\n\nfinal_model= run_sklearn_model('RandomForest',\n                                         params,\n                                         X_train_processed,y_train,\n                                         is_regression=False,\n                                         class_names=['no','yes'],\n                                         test_split=None,\n                                         seed=42,\n                                         plot_fea_imp=True)\n\n------------------------------ Train set ------------------------------\nLog loss: 0.4134397147185217\n              precision    recall  f1-score   support\n\n          no       0.82      0.90      0.86       400\n         yes       0.78      0.63      0.70       214\n\n    accuracy                           0.81       614\n   macro avg       0.80      0.77      0.78       614\nweighted avg       0.80      0.81      0.80       614\n\n\n\n\n\n\n\n\n\n\nInsulin is not an impact feature for this dataset. Digging deeper on the source of the data, there’s a description that said: “This population has minimal European admixture, and their diabetes appears to be exclusively type 2 diabetes, with no evidence of the autoimmunity characteristic of type 1 diabetes”. Type 2 diabetes patients can still produce insulin, but the body is unable to use it. This information about insulin can be useful later if we want to perform feature selection, in order to boost this model’s performance",
    "crumbs": [
      "End-to-end tutorial - Classification"
    ]
  },
  {
    "objectID": "end_to_end_example.html#partial-dependency-plot",
    "href": "end_to_end_example.html#partial-dependency-plot",
    "title": "End-to-end tutorial - Classification",
    "section": "Partial Dependency Plot",
    "text": "Partial Dependency Plot\n\nnum_features = X_train_processed.columns.tolist()\nnum_features\n\n['Pregnancies',\n 'Glucose',\n 'BloodPressure',\n 'SkinThickness',\n 'Insulin',\n 'BMI',\n 'DiabetesPedigreeFunction',\n 'Age']\n\n\n\npdp_numerical_only(final_model,X_train_processed,num_features,class_names=['no diabetes','diabetes'],\n                  nrows=4,ncols=2)\n# y_class is ignored in binary classification or classical regression settings.\n\n\n\n\n\n\n\n\nFor Partial Dependency Plot, Glucose, BMI and Age show the most changes compared to other features. This matches the feature importances plot earlier.\n\nplot_ice_pair(final_model,X_train_processed,pair_features=['Glucose','Age'],class_idx=1,figsize=(10,4))\n# class_idx is ignored in binary classification or classical regression settings.",
    "crumbs": [
      "End-to-end tutorial - Classification"
    ]
  },
  {
    "objectID": "ml_helpers.html",
    "href": "ml_helpers.html",
    "title": "ml_helpers",
    "section": "",
    "text": "source\n\nrun_logistic_regression\n\n run_logistic_regression (X_trn:pandas.core.frame.DataFrame,\n                          y_trn:pandas.core.series.Series|numpy.ndarray,\n                          multi_class='multinomial', solver='newton-cg',\n                          penalty=None, max_iter=10000, return_coef=False)\n\nPerform Sklearn logistic regression, then print coefficients and classification report\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nX_trn\npd.DataFrame\n\nTraining dataframe\n\n\ny_trn\npd.Series | np.ndarray\n\nTraining label\n\n\nmulti_class\nstr\nmultinomial\nsklearn’s log reg multiclass option\n\n\nsolver\nstr\nnewton-cg\nsklearn’s log reg solver option\n\n\npenalty\nNoneType\nNone\nsklearn’s log reg penalty option\n\n\nmax_iter\nint\n10000\nsklearn’s log reg max iteration option\n\n\nreturn_coef\nbool\nFalse\nwhether to return coefficients\n\n\n\n\nsource\n\n\nrun_multinomial_statmodel\n\n run_multinomial_statmodel (X_trn:pandas.core.frame.DataFrame,\n                            y_trn:pandas.core.series.Series|numpy.ndarray,\n                            add_constant=True)\n\nPerform multinominal logit from statsmodel, then print results and classification report\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nX_trn\npd.DataFrame\n\nTraining dataframe\n\n\ny_trn\npd.Series | np.ndarray\n\nTraining label\n\n\nadd_constant\nbool\nTrue\nTo add a constant column to X_trn\n\n\n\n\nsource\n\n\nrun_sklearn_model\n\n run_sklearn_model (model_name:str, model_params:dict,\n                    X_trn:pandas.core.frame.DataFrame,\n                    y_trn:pandas.core.series.Series|numpy.ndarray,\n                    is_regression=False, class_names:list=None,\n                    test_split=None, metric_funcs={}, seed=42,\n                    plot_fea_imp=True)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel_name\nstr\n\nsklearn’s Machine Learning model to try. Currently support DecisionTree,AdaBoost,RandomForest\n\n\nmodel_params\ndict\n\nA dictionary containing model’s hyperparameters\n\n\nX_trn\npd.DataFrame\n\nTraining dataframe\n\n\ny_trn\npd.Series | np.ndarray\n\nTraining label\n\n\nis_regression\nbool\nFalse\nTo use regression model or classification model\n\n\nclass_names\nlist\nNone\nList of names associated with the labels (same order); e.g. [‘no’,‘yes’]. For classification only\n\n\ntest_split\nNoneType\nNone\nTest set split. If float: random split. If list of list: indices of train and test set. If None: skip splitting\n\n\nmetric_funcs\ndict\n{}\nDictionary of metric functions: {metric_name:metric_func}\n\n\nseed\nint\n42\nRandom seed\n\n\nplot_fea_imp\nbool\nTrue\nTo whether plot sklearn’s feature importances. Set to False to skip the plot\n\n\n\n\nsource\n\n\ntune_sklearn_model\n\n tune_sklearn_model (model_name:str, param_grid:dict,\n                     X_trn:pandas.core.frame.DataFrame,\n                     y_trn:pandas.core.series.Series|numpy.ndarray,\n                     is_regression=False, custom_cv=5,\n                     random_cv_iter=None, scoring=None, seed=42,\n                     rank_show=10, show_split_scores=True)\n\nPerform either Sklearn’s Grid Search or Randomized Search (based on random_cv_iter) of the model using param_grid\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel_name\nstr\n\nsklearn’s Machine Learning model to try. Currently support DecisionTree,AdaBoost,RandomForest,\n\n\nparam_grid\ndict\n\nDictionary with parameters names (str) as keys and lists of parameter settings to try as values\n\n\nX_trn\npd.DataFrame\n\nTraining dataframe\n\n\ny_trn\npd.Series | np.ndarray\n\nTraining label\n\n\nis_regression\nbool\nFalse\nIs it a regression problem, or classification?\n\n\ncustom_cv\nint\n5\nsklearn’s cross-validation splitting strategy\n\n\nrandom_cv_iter\nNoneType\nNone\nNumber of parameter settings that are sampled. Use this if you want to do RandomizedSearchCV\n\n\nscoring\nNoneType\nNone\nMetric\n\n\nseed\nint\n42\nRandom seed\n\n\nrank_show\nint\n10\nNumber of ranks to show (descending order)\n\n\nshow_split_scores\nbool\nTrue\nTo show both train and test split scores\n\n\n\n\nsource\n\n\nget_adaboost_info\n\n get_adaboost_info (dt_params, ada_params, X, y, seed=42)\n\n\nsource\n\n\nshow_both_cv\n\n show_both_cv (search_cv, default_cv, scoring, top_n=10,\n               show_split_scores=False)\n\n\nsource\n\n\nsummarize_default_cv\n\n summarize_default_cv (default_cv, s)\n\n\nsource\n\n\nsummarize_cv_results\n\n summarize_cv_results (search_cv, scoring, top_n=10,\n                       show_split_scores=False)\n\n\nsource\n\n\ndo_param_search\n\n do_param_search (X_train, y_train, estimator, param_grid,\n                  random_cv_iter=None, include_default=True, cv=None,\n                  scoring=None, seed=42)",
    "crumbs": [
      "ml_helpers"
    ]
  },
  {
    "objectID": "data_preprocess.html",
    "href": "data_preprocess.html",
    "title": "data_preprocess",
    "section": "",
    "text": "source\n\nprocess_missing_values\n\n process_missing_values (X_train:pandas.core.frame.DataFrame,\n                         X_test:pandas.core.frame.DataFrame=None,\n                         missing_cols:list|str=[],\n                         missing_vals:list|int|float|str=nan,\n                         strategies:list|str='median', **kwargs)\n\nProcess columns with missing values using Sklearn SimpleInputer\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nX_train\npd.DataFrame\n\nTraining dataframe\n\n\nX_test\npd.DataFrame\nNone\nTesting dataframe\n\n\nmissing_cols\nlist | str\n[]\nA column name having missing values, or a list of such columns\n\n\nmissing_vals\nlist | int | float | str\nnan\nA placeholder for missing values, or a list of placeholders for all columns in miss_cols\n\n\nstrategies\nlist | str\nmedian\nThe imputation strategy from sklearn, or a list of such values. Currently support ‘median’,‘mean’,‘most_frequent’\n\n\nkwargs\n\n\n\n\n\n\n\ndf = pd.DataFrame([[7, 2, 3], [4, np.nan, 6], [10, 5, -1]],columns=['col1','col2','col3'])\ndisplay(df)\n\n\n\n\n\n\n\n\ncol1\ncol2\ncol3\n\n\n\n\n0\n7\n2.0\n3\n\n\n1\n4\nNaN\n6\n\n\n2\n10\n5.0\n-1\n\n\n\n\n\n\n\n\ndf_processed = process_missing_values(df,missing_cols=['col2','col3'],missing_vals=[np.NaN,-1],strategy='mean')\ndisplay(df_processed)\n\n\n\n\n\n\n\n\ncol1\ncol2\ncol3\n\n\n\n\n0\n7\n2.0\n3.0\n\n\n1\n4\n3.5\n6.0\n\n\n2\n10\n5.0\n4.5\n\n\n\n\n\n\n\n\ndf_trn = pd.DataFrame([[7, 2, 3], [4, np.nan, 6], [10, 5, -1]],columns=['col1','col2','col3'])\ndf_test = pd.DataFrame([[2, np.NaN, 3], [3, 1, -1]],columns=['col1','col2','col3'])\ndisplay(df_trn,df_test)\n\n\n\n\n\n\n\n\ncol1\ncol2\ncol3\n\n\n\n\n0\n7\n2.0\n3\n\n\n1\n4\nNaN\n6\n\n\n2\n10\n5.0\n-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncol1\ncol2\ncol3\n\n\n\n\n0\n2\nNaN\n3\n\n\n1\n3\n1.0\n-1\n\n\n\n\n\n\n\n\ndf_processed_trn,df_procesed_val= process_missing_values(df_trn,\n                                                         df_test,\n                                                         missing_cols=['col2','col3'],\n                                                         missing_vals=[np.NaN,-1],strategy='mean')\ndisplay(df_processed_trn,df_procesed_val)\n\n\n\n\n\n\n\n\ncol1\ncol2\ncol3\n\n\n\n\n0\n7\n2.0\n3.0\n\n\n1\n4\n3.5\n6.0\n\n\n2\n10\n5.0\n4.5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncol1\ncol2\ncol3\n\n\n\n\n0\n2\n3.5\n3.0\n\n\n1\n3\n1.0\n4.5\n\n\n\n\n\n\n\n\nsource\n\n\nscale_num_cols\n\n scale_num_cols (X_train:pandas.core.frame.DataFrame,\n                 X_test:pandas.core.frame.DataFrame=None,\n                 num_cols:list|str=[], scale_methods:list|str='minmax',\n                 **kwargs)\n\nScale numerical columns using Sklearn\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nX_train\npd.DataFrame\n\nTraining dataframe\n\n\nX_test\npd.DataFrame\nNone\nTesting dataframe\n\n\nnum_cols\nlist | str\n[]\nName of the numerical column, or a list of such columns\n\n\nscale_methods\nlist | str\nminmax\nSklearn scaling method (‘minmax’ or ‘standard’), or a list of such methods\n\n\nkwargs\n\n\n\n\n\n\n\ndf = pd.DataFrame([[7, 2, 3], [4, 2, 6], [10, 5, 1]],columns=['col1','col2','col3'])\ndisplay(df)\n\n\n\n\n\n\n\n\ncol1\ncol2\ncol3\n\n\n\n\n0\n7\n2\n3\n\n\n1\n4\n2\n6\n\n\n2\n10\n5\n1\n\n\n\n\n\n\n\n\ndf_processed = scale_num_cols(df,num_cols=['col1','col3'],scale_methods='standard')\ndisplay(df_processed)\n\n\n\n\n\n\n\n\ncol1\ncol2\ncol3\n\n\n\n\n0\n0.000000\n2\n-0.162221\n\n\n1\n-1.224745\n2\n1.297771\n\n\n2\n1.224745\n5\n-1.135550\n\n\n\n\n\n\n\n\nsource\n\n\none_hot_cat\n\n one_hot_cat (X_train:pandas.core.frame.DataFrame,\n              X_test:pandas.core.frame.DataFrame=None,\n              cat_cols:list|str=[], bi_cols:list|str=[], **kwargs)\n\nPerform ‘get_dummies’ on categorical columns\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nX_train\npd.DataFrame\n\nTraining dataframe\n\n\nX_test\npd.DataFrame\nNone\nTesting dataframe\n\n\ncat_cols\nlist | str\n[]\nName of the categorical columns (non-binary), or a list of such columns\n\n\nbi_cols\nlist | str\n[]\nName of the binary column, or a list of such columns\n\n\nkwargs\n\n\n\n\n\n\n\ndf = pd.DataFrame({'A': ['a', 'b', 'a'], 'B': ['b', 'a', 'c'],\n                   'C': [1, 2, 3]})\ndisplay(df)\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\na\nb\n1\n\n\n1\nb\na\n2\n\n\n2\na\nc\n3\n\n\n\n\n\n\n\n\ndf_processed = one_hot_cat(df,cat_cols='B',bi_cols='A')\ndisplay(df_processed)\n\n\n\n\n\n\n\n\nC\nB_a\nB_b\nB_c\nA_b\n\n\n\n\n0\n1\n0.0\n1.0\n0.0\n0.0\n\n\n1\n2\n1.0\n0.0\n0.0\n1.0\n\n\n2\n3\n0.0\n0.0\n1.0\n0.0\n\n\n\n\n\n\n\n\nsource\n\n\npreprocessing_general\n\n preprocessing_general (X_train:pandas.core.frame.DataFrame,\n                        X_test:pandas.core.frame.DataFrame=None, **kwargs)\n\n*The main preprocessing functions, will perform:\n\nFill missing values\nScale numerical columns\nOne-hot encode categorical columns\n\nRemember to put in the appropriate keyword arguments for each of the preprocessings mentioned above*\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nX_train\npd.DataFrame\n\nTraining dataframe\n\n\nX_test\npd.DataFrame\nNone\nTesting dataframe\n\n\nkwargs\n\n\n\n\n\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/titanic.csv')\n\n\n# Select some useful features, for now\ndf = df[['Survived','Pclass','Sex','Age','SibSp','Parch','Embarked']].copy()\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 156 entries, 0 to 155\nData columns (total 7 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Survived  156 non-null    int64  \n 1   Pclass    156 non-null    int64  \n 2   Sex       156 non-null    object \n 3   Age       126 non-null    float64\n 4   SibSp     156 non-null    int64  \n 5   Parch     156 non-null    int64  \n 6   Embarked  155 non-null    object \ndtypes: float64(1), int64(4), object(2)\nmemory usage: 8.7+ KB\n\n\n\ndf.sample(5)\n\n\n\n\n\n\n\n\nSurvived\nPclass\nSex\nAge\nSibSp\nParch\nEmbarked\n\n\n\n\n84\n1\n2\nfemale\n17.0\n0\n0\nS\n\n\n4\n0\n3\nmale\n35.0\n0\n0\nS\n\n\n101\n0\n3\nmale\nNaN\n0\n0\nS\n\n\n48\n0\n3\nmale\nNaN\n2\n0\nC\n\n\n112\n0\n3\nmale\n22.0\n0\n0\nS\n\n\n\n\n\n\n\nLet’s perform a simple train/test split\n\nfrom sklearn.model_selection import train_test_split\n\n\nX_train, X_test, y_train, y_test = train_test_split(df.drop('Survived',axis=1), df['Survived'],\n                                                    test_size=0.2,\n                                                    random_state=42,\n                                                    stratify=df['Survived'])\n\n\nX_train.head()\n\n\n\n\n\n\n\n\nPclass\nSex\nAge\nSibSp\nParch\nEmbarked\n\n\n\n\n142\n3\nfemale\n24.0\n1\n0\nS\n\n\n134\n2\nmale\n25.0\n0\n0\nS\n\n\n120\n2\nmale\n21.0\n2\n0\nS\n\n\n50\n3\nmale\n7.0\n4\n1\nS\n\n\n133\n2\nfemale\n29.0\n1\n0\nS\n\n\n\n\n\n\n\n\nX_test.head()\n\n\n\n\n\n\n\n\nPclass\nSex\nAge\nSibSp\nParch\nEmbarked\n\n\n\n\n91\n3\nmale\n20.0\n0\n0\nS\n\n\n145\n2\nmale\n19.0\n1\n1\nS\n\n\n115\n3\nmale\n21.0\n0\n0\nS\n\n\n106\n3\nfemale\n21.0\n0\n0\nS\n\n\n9\n2\nfemale\n14.0\n1\n0\nC\n\n\n\n\n\n\n\n\nX_train_processed,X_test_processed = preprocessing_general(X_train,X_test,\n                                                           missing_cols=['Age','Embarked'],\n                                                           missing_vals=np.NaN,\n                                                           strategies=['median','most_frequent'],\n                                                           num_cols=['Age','SibSp','Parch'],\n                                                           scale_methods=['standard','minmax','minmax'],\n                                                           cat_cols='Embarked',\n                                                           bi_cols='Sex'\n                                                          )\n\nNotice that I don’t add Pclass to the preprocessing function. That means this column will be left untouched\n\nX_train_processed.head()\n\n\n\n\n\n\n\n\nPclass\nAge\nSibSp\nParch\nEmbarked_C\nEmbarked_Q\nEmbarked_S\nSex_male\n\n\n\n\n142\n3\n-0.325526\n0.2\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n134\n2\n-0.252796\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n\n\n120\n2\n-0.543716\n0.4\n0.0\n0.0\n0.0\n1.0\n1.0\n\n\n50\n3\n-1.561938\n0.8\n0.2\n0.0\n0.0\n1.0\n1.0\n\n\n133\n2\n0.038125\n0.2\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n\n\n\n\n\n\nX_test_processed.head()\n\n\n\n\n\n\n\n\nPclass\nAge\nSibSp\nParch\nEmbarked_C\nEmbarked_Q\nEmbarked_S\nSex_male\n\n\n\n\n91\n3\n-0.616446\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n\n\n145\n2\n-0.689176\n0.2\n0.2\n0.0\n0.0\n1.0\n1.0\n\n\n115\n3\n-0.543716\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n\n\n106\n3\n-0.543716\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n9\n2\n-1.052827\n0.2\n0.0\n1.0\n0.0\n0.0\n0.0",
    "crumbs": [
      "data_preprocess"
    ]
  }
]